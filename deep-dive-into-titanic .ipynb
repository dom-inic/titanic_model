{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nimport tarfile \nimport pandas as pd\n\n# load the dataset \ndef load_titanic_data(titanic_path=os.path.join(\"datasets\", \"titanic\")):\n    csv_path = os.path.join(titanic_path, \"train.csv\")\n    return pd.read_csv(csv_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntitanic_train = pd.read_csv('../input/titanic/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the head functions to get the fist five rows of the data\ntitanic_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the info function to know how large our dataset is \n# and checking for noiseness in our dataset\ntitanic_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_train[\"Cabin\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_train[\"Embarked\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_train[\"Sex\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_train[\"Pclass\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  attributes[sex, cabin, embarked, age, ]\n# plottng the data using a histogram to show the number of instances on the y and x gven range on the \nimport matplotlib.pyplot as plt\ntitanic_train.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression task = binary classifier\n# what is the probability that object a survived the shipreck based on it properties[sex, age, pclass, cabin, \n#  seems our train data has some missing values in the age attribute, cabin attribute and the Embarked attribute\n# we are gonna have to transform some of the attributes to text for our machine learning algos\n# using scikit-learn LabelEncoder class \nfrom sklearn.linear_model import LogisticRegression\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pands scatter_matrix funcrion to check for correlation btn attributes\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"Sex\", \"Age\", \"Cabin\",\n             \"Survived\", \"Pclass\"]\nscatter_matrix(titanic_train[attributes], figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at the most promising attributes that can help us predict\n# Age , Sex\ntitanic_train.plot(kind=\"hist\", x=\"Survived\", y=\"Age\", \n            alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualisations for Pclass against survived \n\ntitanic_train.plot(kind=\"hist\", x=\"Pclass\", y=\"Survived\",  \n            alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from the above output we need to transform some of the data that's of string type to num\n# most machine learning algorithms work with data that's off num type . \n# attributes to transform : Sex, Pclass, Survived\n# there are some attributes that we are going to drop since they dont help us e.g Name, Ticket, Fare, Parch, Sibsp\n# droppping the unimportant attributes using the drop function or dropna\n# converting some of the data to number formart using scikit learn's LabelEncoder class\n# after which we will put the data in a pipeline and later feed it to machine learning algorithms\n\nimport numpy as np\nnp.random.seed(10)\nimport seaborn as sns\n%matplotlib inline \nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# thought of using a perceptron neural network\n# multilayer perceptron\n\n# so first lets convert some of the data that is in string formart\n# defining a dictionary to binarize the sex\n\ndict_sex = {\n    'male': 0,\n    'female': 1\n}\n\n# defining a dictionary to transform the 0, 1 values in survived attribute to num\ndict_live = {\n    0: 'perished', \n    1: 'survived'\n}\n\n# let's apply the dictionary using a lambda function \ntitanic_train['Bsex'] = titanic_train['Sex'].apply(lambda x : dict_sex[x])\n\n# now we havee a new attribute Bsex\n\n# features are a 2 column matrix \nfeatures = titanic_train[['Pclass', 'Bsex']].to_numpy()\nlabels = titanic_train['Survived'].to_numpy()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Artificial Neural network\n# we will define sigmoid and Relu activation functions\n\n# sigmoid \n# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\ndef sigmoid_act(x, der=False):\n    import numpy as np\n    \n    if (der==True) : #derivative of the sigmoid\n        f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n    else : # sigmoid\n        f = 1/(1+ np.exp(- x))\n    \n    return f\n\n\n# Rectifier Linear Unit (ReLU)\ndef ReLU_act(x, der=False):\n    import numpy as np\n    \n    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n        f = np.heaviside(x, 1)\n    else :\n        f = np.maximum(x, 0)\n    \n    return f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_train.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting into train sets and test sets using sklearn \n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.30)\n\nprint('Training records:',Y_train.size)\nprint('Test records:',Y_test.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our multilayer perceptron \n# eta is the learning rate\n# p & q are our number of perceptrons \ndef Multilayer_perceptron(X_train, Y_train, p=4, q=4, eta=0.0015):\n    \n    # 0: Random initialize the relevant data for our three layers\n#     1st hidden layer\n\n    w1 = 2*np.random.rand(p , X_train.shape[1]) - 0.5 \n    b1 = np.random.rand(p)\n\n    w2 = 2*np.random.rand(q , p) - 0.5  # Layer 2\n    b2 = np.random.rand(q)\n\n    wOut = 2*np.random.rand(q) - 0.5   # Output Layer\n    bOut = np.random.rand(1)\n\n    mu = []\n    vec_y = []\n\n    # Start looping over the passengers, i.e. over I.\n\n    for I in range(0, X_train.shape[0]-1): #loop in all the passengers:\n    \n        # 1: input the data \n        x = X_train[I]\n    \n        # 2: Start the algorithm\n    \n        # 2.1: Feed forward\n        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n        y = sigmoid_act(np.dot(wOut, z2) + bOut) # Output of the Output layer\n    \n        #2.2: Compute the output layer's error\n        delta_Out = 2 * (y-Y_train[I]) * sigmoid_act(y, der=True)\n    \n        #2.3: Backpropagate\n        delta_2 = delta_Out * wOut * ReLU_act(z2, der=True) # Second Layer Error\n        delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True) # First Layer Error\n    \n        # 3: Gradient descent \n        wOut = wOut - eta*delta_Out*z2  # Outer Layer\n        bOut = bOut - eta*delta_Out\n    \n        w2 = w2 - eta*np.kron(delta_2, z1).reshape(q,p) # Hidden Layer 2\n        b2 = b2 -  eta*delta_2\n    \n        w1 = w1 - eta*np.kron(delta_1, x).reshape(p, x.shape[0])\n        b1 = b1 - eta*delta_1\n    \n        # 4. Computation of the loss function\n        mu.append((y-Y_train[I])**2)\n        vec_y.append(y)\n    \n    batch_loss = []\n    for i in range(0, 10):\n        loss_avg = 0\n        for m in range(0, 60):\n            loss_avg+=vec_y[60*i+m]/60\n        batch_loss.append(loss_avg)\n    \n    \n    plt.figure(figsize=(10,6))\n    plt.scatter(np.arange(1, len(batch_loss)+1), batch_loss, alpha=1, s=10, label='error')\n    plt.title('Averege Loss by epoch', fontsize=20)\n    plt.xlabel('Epoch', fontsize=16)\n    plt.ylabel('Loss', fontsize=16)\n    plt.show()\n    \n    return w1, b1, w2, b2, wOut, bOut, mu\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w1, b1, w2, b2, wOut, bOut, mu = Multilayer_perceptron(X_train, Y_train, p=8, q=4, eta=0.0015)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# computing predictions \n# using weights and biases to compute predictions \n\ndef MLP_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu):\n    import numpy as np\n    \n    pred = []\n    \n    for I in range(0, X_test.shape[0]): #loop in all the passengers\n        # 1: input the data \n        x = X_test[I]\n        \n        # 2.1: Feed forward\n        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n        y = sigmoid_act(np.dot(wOut, z2) + bOut)  # Output of the Output layer\n        \n        # Append the prediction;\n        # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n        # if y < 0.5 the output is zero, otherwise is 1\n        pred.append( np.heaviside(y - 0.5, 1)[0] )\n    \n    \n    return np.array(pred);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = MLP_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizations \n# plotting the confusion matrix \n\ncm = confusion_matrix(Y_test, preds)\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tesing our MLP on the test dataset \n# loading the test dataset \n\ntitanic_test = pd.read_csv('../input/titanic/test.csv')\n\ntitanic_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# binarizing the sex attribute\n# extracting the important attributes that we will be using \n\ntitanic_test['Bsex'] = titanic_test['Sex'].apply(lambda x : dict_sex[x])\n\n\nX = titanic_test[['Pclass', 'Bsex']].to_numpy()\n\ntest_preds = MLP_pred(X, w1, b1, w2, b2, wOut, bOut, mu)\ntest_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# exporting the predictions as csv\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": test_preds\n    })\n\nsubmission.head(5)\n\n# Export it in a 'Comma Separated Values' (CSV) file\nimport os\nos.chdir(r'../working')\nsubmission.to_csv(r'submission.csv', index=False)\n# Creating a link to download the .csv file we created\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submitted_data = pd.read_csv('../working/submission.csv')\nsubmitted_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submitted_data[\"Survived\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualisations for Pclass against survived \n\nsubmitted_data.plot(kind=\"hist\", y=\"Survived\",  \n            alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From the above output our perceptron predicted that out of the 410 sample people from the titanic\n# only 80 of the survived while the rest 338 perished ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}